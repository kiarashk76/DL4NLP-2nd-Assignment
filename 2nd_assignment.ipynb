{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiarashk76/DL4NLP-2nd-Assignment/blob/master/2nd_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f43EbqbDkfue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mounting the google drive for accessing the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqDbnd9iksVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/Datasets/imdb-movie-reviews-dataset.zip' '/content/'\n",
        "!unzip imdb-movie-reviews-dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNGhAlNKkujh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATA_PATH = '/content/aclImdb'\n",
        "\n",
        "# listing training directories\n",
        "train_pos = os.listdir(os.path.join(DATA_PATH, 'train/pos'))\n",
        "for i in range(len(train_pos)):\n",
        "  train_pos[i] = 'pos/' + train_pos[i] \n",
        "  \n",
        "train_neg = os.listdir(os.path.join(DATA_PATH, 'train/neg'))\n",
        "for i in range(len(train_neg)):\n",
        "  train_neg[i] = 'neg/' + train_neg[i]\n",
        "\n",
        "# listing test directories\n",
        "test_pos = os.listdir(os.path.join(DATA_PATH, 'test/pos'))\n",
        "for i in range(len(test_pos)):\n",
        "  test_pos[i] = 'pos/' + test_pos[i] \n",
        "  \n",
        "test_neg = os.listdir(os.path.join(DATA_PATH, 'test/neg'))\n",
        "for i in range(len(test_neg)):\n",
        "  test_neg[i] = 'neg/' + test_neg[i]\n",
        "  \n",
        "# extracting validation directories from training\n",
        "np.random.shuffle(train_pos)\n",
        "np.random.shuffle(train_neg)\n",
        "\n",
        "val_pos = train_pos[0:2500]\n",
        "val_neg = train_neg[0:2500]\n",
        "\n",
        "del train_pos[0:2500]\n",
        "del train_neg[0:2500]\n",
        "\n",
        "# shuffling pos and neg trainings\n",
        "X_train = train_pos +  train_neg \n",
        "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "\n",
        "np.random.seed(314)\n",
        "np.random.shuffle(X_train) \n",
        "np.random.seed(314) \n",
        "np.random.shuffle(y_train)\n",
        "\n",
        "# creating pos and neg validations\n",
        "X_val = val_pos +  val_neg \n",
        "y_val = [1] * len(val_pos) + [0] * len(val_neg)\n",
        "\n",
        "# creating pos and neg tests\n",
        "X_test = test_pos +  test_neg \n",
        "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1QlNEVDkxdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_id2word(vocab):\n",
        "  results = []\n",
        "  for i in vocab:\n",
        "    results.append(i[0])\n",
        "  return results\n",
        "\n",
        "def create_word2id(vocab):\n",
        "  results = dict()\n",
        "  for i in range(len(vocab)):\n",
        "    results[vocab[i][0]] = i\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeV4CYaJkzwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = dict()\n",
        "\n",
        "# extract the 2000 most frequent words in our dataset\n",
        "for address in X_train:\n",
        "  file = open(os.path.join(DATA_PATH, 'train', address), 'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in vocab:\n",
        "      vocab[w] += 1\n",
        "    else:\n",
        "      vocab[w] = 1\n",
        "\n",
        "\n",
        "vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = vocab[0:2000]\n",
        "\n",
        "id2word = create_id2word(vocab)\n",
        "word2id = create_word2id(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjSlEPZk17a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract feature vectors for training\n",
        "\n",
        "x_train = np.zeros( (len(X_train),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_train:\n",
        "  file = open(os.path.join(DATA_PATH, 'train', address), 'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_train[i][word2id[w]] = 1\n",
        "  i += 1\n",
        "  \n",
        "# extract feature vectors for validation\n",
        "x_val = np.zeros( (len(X_val),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_val:\n",
        "  file = open(os.path.join(DATA_PATH, 'train', address), 'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_val[i][word2id[w]] = 1\n",
        "  i += 1\n",
        "  \n",
        "# extract feature vectors for test\n",
        "x_test = np.zeros( (len(X_test),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_test:\n",
        "  file = open(os.path.join(DATA_PATH, 'test', address), 'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_test[i][word2id[w]] = 1\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TALz1nYk3yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "# save the data into drive\n",
        "pickle.dump([x_train, y_train, x_val, y_val, x_test, y_test], open(\"data.pkl\", \"wb\")) \n",
        "pickle.dump([id2word, word2id],  open(\"aux.pkl\", \"wb\"))\n",
        "\n",
        "# load from the saved \n",
        "# x_train, y_train, x_val, y_val, x_test, y_test = pickle.load( open(\"data.pkl\", \"rb\"))\n",
        "# id2word, word2id = pickle.load( open(\"aux.pkl\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjtxHB98wFNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derive(x):\n",
        "  return sigmoid(x)* (1-sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZZVX8tOwUlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class layer():\n",
        "  \"\"\"\n",
        "  This class is for simulating each layer of a NN model: \n",
        "  - input\n",
        "  - weights\n",
        "  - activation function\n",
        "  - derivative of activation function\n",
        "  - output before activation\n",
        "  - output after activation\n",
        "  \"\"\"\n",
        "  def __init__(self, shape, act_function):\n",
        "    self.W = np.random.uniform(-0.5 ,0.5 , shape)\n",
        "    self.b = np.random.uniform(-0.5 ,0.5 , shape[1])\n",
        "    self.grad = np.random.uniform(-0.5, 0.5, shape)\n",
        "    self.act_function = act_function\n",
        "  def activation_function(self, x):\n",
        "    if self.act_function == \"sigmoid\":\n",
        "      return sigmoid(x)\n",
        "\n",
        "  def activation_function_derivative(self, x):\n",
        "    if self.act_function == \"sigmoid\":\n",
        "      return sigmoid_derive(x)\n",
        "  \n",
        "  def calculate_output(self, X):\n",
        "    self.x = X\n",
        "    self.z = np.dot(X, self.W) + self.b \n",
        "    self.y = self.activation_function(self.z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVx1FiGAxikP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model():\n",
        "  \"\"\"\n",
        "  This class is for simulating a multi layer model using the Layer class. Methods:\n",
        "  - set train / validation / test data\n",
        "  - add layers without limitation (ofcourse there is a time limitation)\n",
        "  - calculate forward propagation\n",
        "  - calculate back propagation for training the model\n",
        "  - testing the model with the best model with regard to validation accuracy\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.layers_list = []\n",
        "    self.best_val_acc = 0\n",
        "\n",
        "  def set_training_data(self, X_train, y_train):\n",
        "    self.X_train = X_train #(20000, 2000)\n",
        "    self.y_train = y_train #(20000, )\n",
        "    \n",
        "  def set_validation_data(self, X_val, y_val):\n",
        "    self.X_val = X_val #(5000, 2000)\n",
        "    self.y_val = y_val #(5000, )\n",
        "    \n",
        "  def set_test_data(self, X_test, y_test):\n",
        "    self.X_test = X_test #(25000,2000) \n",
        "    self.y_test = y_test #(25000)\n",
        "  \n",
        "  def set_input_layer(self, input_features):\n",
        "    self.input_features = input_features\n",
        "\n",
        "  def set_output_layer(self, output_num):\n",
        "    self.output_num = output_num\n",
        "\n",
        "  def add_layer(self, weights_num, act_function = \"sigmoid\"):\n",
        "    # add a layer with the number of neurons given in the parameter\n",
        "    if len(self.layers_list) == 0:\n",
        "      input_shape = self.X_train.shape\n",
        "      l = layer([self.input_features, weights_num], act_function)\n",
        "      self.layers_list.append(l)\n",
        "    else:\n",
        "      last_layer_shape = self.layers_list[-1].W.shape\n",
        "      l = layer([last_layer_shape[1], weights_num], act_function)\n",
        "      self.layers_list.append(l)\n",
        "\n",
        "  def forward_propagation(self, X):\n",
        "    # calculating the forward propagation and saving \n",
        "    # the middle point data in the layers\n",
        "    if len(self.layers_list) == 0:\n",
        "      print(\"No layers exist\")\n",
        "      return 0 \n",
        "    for i,lay in enumerate(self.layers_list):\n",
        "      if i == 0:\n",
        "        lay.calculate_output(X)\n",
        "      else:\n",
        "        last_layer_output = self.layers_list[i-1].y\n",
        "        lay.calculate_output(last_layer_output)\n",
        "    \n",
        "  def train(self, batch_size = 20, learning_rate = 0.1, num_epochs = 10):\n",
        "    self.add_layer(self.output_num)\n",
        "    total_number_of_batches = int(self.X_train.shape[0] / batch_size)\n",
        "    self.loss_list = []\n",
        "    self.acc_train = []\n",
        "    self.acc_val = []\n",
        "    for i in range(num_epochs):\n",
        "      print(\"epoch \",i,\"-> \",end='')\n",
        "      total_loss = 0\n",
        "      for j in range(total_number_of_batches):\n",
        "        X = self.X_train[batch_size*j : batch_size* (j+1)] # (batch_size, 2000)\n",
        "        y = self.y_train[batch_size*j : batch_size* (j+1)] # (batch_size, )\n",
        "\n",
        "        self.forward_propagation(X)\n",
        "        \n",
        "        # calculating derivatives with respect to layers\n",
        "        for i in range(len(self.layers_list) - 1, -1, -1):\n",
        "          current_layer = self.layers_list[i]    \n",
        "          if i == len(self.layers_list) - 1:\n",
        "            if  (current_layer.y * ((current_layer.y) - 1)).all() != 0:\n",
        "              current_layer.grad = (y[:, None] - current_layer.y) / (current_layer.y * ((current_layer.y) - 1))\n",
        "          else:\n",
        "            last_layer = self.layers_list[i+1]\n",
        "            current_layer.grad = np.dot(last_layer.grad * last_layer.activation_function_derivative(last_layer.z), last_layer.W.T)\n",
        "\n",
        "        # calculating derivatives with respect to weights\n",
        "        for i in range(len(self.layers_list)):\n",
        "          current_layer = self.layers_list[i]\n",
        "          gradW = np.dot(current_layer.x.T, current_layer.grad * current_layer.activation_function_derivative(current_layer.z)) / current_layer.grad.shape[0]\n",
        "          gradb = np.mean(current_layer.grad * current_layer.activation_function_derivative(current_layer.z))\n",
        "\n",
        "          current_layer.W -= learning_rate * gradW\n",
        "          current_layer.b -= learning_rate * gradb\n",
        "      \n",
        "      # calculating training & validation accuracy\n",
        "      train_acc = self.calculate_accuracy(self.X_train, self.y_train, self)\n",
        "      val_acc = self.calculate_accuracy(self.X_val, self.y_val, self)\n",
        "      self.acc_train.append(train_acc)\n",
        "      self.acc_val.append(val_acc)\n",
        "\n",
        "      # saving the best model with regard to validation accuracy\n",
        "      if (val_acc > self.best_val_acc):\n",
        "        self.best_val_acc = val_acc\n",
        "        self.save_model()                 \n",
        "      print(\"train acc:\", train_acc, \"validation acc:\",val_acc)\n",
        "  \n",
        "  def predict_label(self, X, model):\n",
        "    # predicting the output for input (X) given the model\n",
        "    model.threshhold = 0.5\n",
        "    model.forward_propagation(X)\n",
        "    prob = model.layers_list[-1].y\n",
        "    predict = np.empty_like(prob)\n",
        "    for i in range(len(prob)):\n",
        "      if prob[i][0] > model.threshhold :\n",
        "        predict[i] = 1\n",
        "      else:\n",
        "        predict[i] = 0\n",
        "    return predict\n",
        "\n",
        "  def plot_accuracy(self):\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(self.acc_train, label= 'train accuracy')\n",
        "    plt.plot(self.acc_val, label= 'validation accuracy')    \n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "  def save_model(self):\n",
        "    # save the current model object\n",
        "    self.saved_model = self\n",
        "\n",
        "  def calculate_accuracy(self, X, y, model):\n",
        "      counter = 0 \n",
        "      prediction = self.predict_label(X, model)\n",
        "      for i in range(len(prediction)):\n",
        "        if prediction[i] == y[i]:\n",
        "          counter += 1\n",
        "      accuracy = counter / len(prediction)\n",
        "      return accuracy\n",
        "  \n",
        "  def test_model(self, X = None, y = None):\n",
        "    # test the best model for the given data\n",
        "    # if no data given to this function test with the preset test data\n",
        "    if (not X or not y):\n",
        "      return self.calculate_accuracy(self.X_test, self.y_test, self.saved_model)\n",
        "    else:\n",
        "      return self.calculate_accuracy(X, y, self.saved_model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxfLw3LZk9qM",
        "colab_type": "code",
        "outputId": "1aa9d762-a1f7-4113-c9cc-07bd4f013cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  m = model()\n",
        "  m.set_training_data(x_train, y_train)\n",
        "  m.set_validation_data(x_val, y_val)\n",
        "  m.set_test_data(x_test, y_test) \n",
        "  \n",
        "  m.set_input_layer(2000)\n",
        "  m.set_output_layer(1)\n",
        "\n",
        "  m.add_layer(200, act_function= \"sigmoid\")\n",
        "\n",
        "  m.train(batch_size = 20, learning_rate = 0.1, num_epochs = 50)\n",
        "  print(\"test accuracy : \", m.test_model())\n",
        "  m.plot_accuracy()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0 -> train acc: 0.7408 validation acc: 0.7326\n",
            "epoch  1 -> train acc: 0.78555 validation acc: 0.7646\n",
            "epoch  2 -> train acc: 0.80865 validation acc: 0.779\n",
            "epoch  3 -> train acc: 0.8229 validation acc: 0.7916\n",
            "epoch  4 -> train acc: 0.83385 validation acc: 0.7988\n",
            "epoch  5 -> train acc: 0.8435 validation acc: 0.8042\n",
            "epoch  6 -> train acc: 0.85135 validation acc: 0.8068\n",
            "epoch  7 -> train acc: 0.8569 validation acc: 0.8088\n",
            "epoch  8 -> train acc: 0.86135 validation acc: 0.8114\n",
            "epoch  9 -> train acc: 0.86585 validation acc: 0.8104\n",
            "epoch  10 -> train acc: 0.86785 validation acc: 0.8094\n",
            "epoch  11 -> train acc: 0.8726 validation acc: 0.8106\n",
            "epoch  12 -> train acc: 0.8768 validation acc: 0.8104\n",
            "epoch  13 -> train acc: 0.88025 validation acc: 0.8096\n",
            "epoch  14 -> train acc: 0.8843 validation acc: 0.8086\n",
            "epoch  15 -> train acc: 0.8888 validation acc: 0.8088\n",
            "epoch  16 -> train acc: 0.8932 validation acc: 0.8074\n",
            "epoch  17 -> train acc: 0.89735 validation acc: 0.8058\n",
            "epoch  18 -> train acc: 0.9027 validation acc: 0.805\n",
            "epoch  19 -> train acc: 0.90715 validation acc: 0.806\n",
            "epoch  20 -> train acc: 0.91205 validation acc: 0.806\n",
            "epoch  21 -> train acc: 0.9186 validation acc: 0.8062\n",
            "epoch  22 -> train acc: 0.92485 validation acc: 0.8076\n",
            "epoch  23 -> train acc: 0.9309 validation acc: 0.8086\n",
            "epoch  24 -> train acc: 0.9379 validation acc: 0.81\n",
            "epoch  25 -> train acc: 0.9439 validation acc: 0.81\n",
            "epoch  26 -> train acc: 0.9493 validation acc: 0.8112\n",
            "epoch  27 -> train acc: 0.95655 validation acc: 0.8132\n",
            "epoch  28 -> train acc: 0.9625 validation acc: 0.8148\n",
            "epoch  29 -> train acc: 0.96905 validation acc: 0.8162\n",
            "epoch  30 -> train acc: 0.9747 validation acc: 0.818\n",
            "epoch  31 -> train acc: 0.9804 validation acc: 0.8218\n",
            "epoch  32 -> train acc: 0.9851 validation acc: 0.8236\n",
            "epoch  33 -> train acc: 0.989 validation acc: 0.8252\n",
            "epoch  34 -> train acc: 0.99155 validation acc: 0.8266\n",
            "epoch  35 -> train acc: 0.9945 validation acc: 0.8274\n",
            "epoch  36 -> train acc: 0.9962 validation acc: 0.828\n",
            "epoch  37 -> train acc: 0.9972 validation acc: 0.8294\n",
            "epoch  38 -> train acc: 0.9985 validation acc: 0.8306\n",
            "epoch  39 -> train acc: 0.99905 validation acc: 0.8296\n",
            "epoch  40 -> train acc: 0.99945 validation acc: 0.8304\n",
            "epoch  41 -> train acc: 0.99965 validation acc: 0.8306\n",
            "epoch  42 -> train acc: 0.9998 validation acc: 0.8312\n",
            "epoch  43 -> train acc: 0.99985 validation acc: 0.8318\n",
            "epoch  44 -> train acc: 0.99985 validation acc: 0.8328\n",
            "epoch  45 -> train acc: 0.99985 validation acc: 0.8334\n",
            "epoch  46 -> train acc: 0.9999 validation acc: 0.8336\n",
            "epoch  47 -> train acc: 0.9999 validation acc: 0.8334\n",
            "epoch  48 -> train acc: 0.9999 validation acc: 0.8326\n",
            "epoch  49 -> train acc: 0.99985 validation acc: 0.833\n",
            "test accuracy :  0.82804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACTCAYAAAB1YlneAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVUXU34QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}