{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiarashk76/DL4NLP-2nd-Assignment/blob/master/2nd_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8A4nSTC0TRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "505bf0c8-9133-409f-8fd2-cb99958b8c65"
      },
      "source": [
        "#mounting the google drive for accessing the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmU5w56A0WbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/Datasets/imdb-movie-reviews-dataset.zip' '/content/'\n",
        "!unzip imdb-movie-reviews-dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUAXWtzz0bic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# listing training directories\n",
        "train_pos = os.listdir('/content/aclImdb/train/pos')\n",
        "for i in range(len(train_pos)):\n",
        "  train_pos[i] = 'pos/' + train_pos[i] \n",
        "  \n",
        "train_neg = os.listdir('/content/aclImdb/train/neg')\n",
        "for i in range(len(train_neg)):\n",
        "  train_neg[i] = 'neg/' + train_neg[i]\n",
        "\n",
        "# listing test directories\n",
        "test_pos = os.listdir('/content/aclImdb/test/pos')\n",
        "for i in range(len(test_pos)):\n",
        "  test_pos[i] = 'pos/' + test_pos[i] \n",
        "  \n",
        "test_neg = os.listdir('/content/aclImdb/test/neg')\n",
        "for i in range(len(test_neg)):\n",
        "  test_neg[i] = 'neg/' + test_neg[i]\n",
        "  \n",
        "# extracting validation directories from training\n",
        "np.random.shuffle(train_pos)\n",
        "np.random.shuffle(train_neg)\n",
        "\n",
        "val_pos = train_pos[0:2500]\n",
        "val_neg = train_neg[0:2500]\n",
        "\n",
        "del train_pos[0:2500]\n",
        "del train_neg[0:2500]\n",
        "\n",
        "# shuffling pos and neg trainings\n",
        "X_train = train_pos +  train_neg \n",
        "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "\n",
        "np.random.seed(314)\n",
        "np.random.shuffle(X_train) \n",
        "np.random.seed(314) \n",
        "np.random.shuffle(y_train)\n",
        "\n",
        "# creating pos and neg validations\n",
        "X_val = val_pos +  val_neg \n",
        "y_val = [1] * len(val_pos) + [0] * len(val_neg)\n",
        "\n",
        "# creating pos and neg tests\n",
        "X_test = test_pos +  test_neg \n",
        "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nC-yGUI0afI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_id2word(vocab):\n",
        "  results = []\n",
        "  for i in vocab:\n",
        "    results.append(i[0])\n",
        "  return results\n",
        "\n",
        "def create_word2id(vocab):\n",
        "  results = dict()\n",
        "  for i in range(len(vocab)):\n",
        "    results[vocab[i][0]] = i\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4XkUEuC0ejy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = dict()\n",
        "\n",
        "# extract the 2000 most frequent words in our dataset\n",
        "for address in X_train:\n",
        "  file = open('/content/aclImdb/train/'+address,'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in vocab:\n",
        "      vocab[w] += 1\n",
        "    else:\n",
        "      vocab[w] = 1\n",
        "\n",
        "\n",
        "vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = vocab[0:2000]\n",
        "\n",
        "id2word = create_id2word(vocab)\n",
        "word2id = create_word2id(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iHVhD020jnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract feature vectors for training\n",
        "\n",
        "x_train = np.zeros( (len(X_train),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_train:\n",
        "  file = open('/content/aclImdb/train/'+address,'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_train[i][word2id[w]] = 1\n",
        "  i += 1\n",
        "  \n",
        "# extract feature vectors for validation\n",
        "x_val = np.zeros( (len(X_val),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_val:\n",
        "  file = open('/content/aclImdb/train/'+address,'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_val[i][word2id[w]] = 1\n",
        "  i += 1\n",
        "  \n",
        "# extract feature vectors for test\n",
        "x_test = np.zeros( (len(X_test),2000), dtype=int)\n",
        "i = 0\n",
        "for address in X_test:\n",
        "  file = open('/content/aclImdb/test/'+address,'r')\n",
        "  txt = file.read().strip().lower()\n",
        "  words = txt.split()\n",
        "  for w in words:\n",
        "    if w in word2id:\n",
        "      x_test[i][word2id[w]] = 1\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibET7ufx0lkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "# save the data into drive\n",
        "pickle.dump([x_train, y_train, x_val, y_val, x_test, y_test], open(\"data.pkl\", \"wb\")) \n",
        "pickle.dump([id2word, word2id],  open(\"aux.pkl\", \"wb\"))\n",
        "\n",
        "# load from the saved \n",
        "# x_train, y_train, x_val, y_val, x_test, y_test = pickle.load( open(\"data.pkl\", \"rb\"))\n",
        "# id2word, word2id = pickle.load( open(\"aux.pkl\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLgCosoe0n_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class logistic_regression():\n",
        "  def __init__(self, batch_size = 20, learning_rate = 0.1, num_epochs = 300, threshhold = 0.5):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_epochs = num_epochs\n",
        "    self.loss_list = []\n",
        "    self.threshhold = threshhold\n",
        "    \n",
        "  def set_training_data(self, X_train, y_train):\n",
        "    self.X_train = X_train #(20000, 2000)\n",
        "    self.y_train = y_train #(20000, )\n",
        "    \n",
        "  def set_validation_data(self, X_val, y_val):\n",
        "    self.X_val = X_val #(5000, 2000)\n",
        "    self.y_val = y_val #(5000, )\n",
        "    \n",
        "  def set_test_data(self, X_test, y_test):\n",
        "    self.X_test = X_test #(25000,2000) \n",
        "    self.y_test = y_test #(25000)\n",
        "\n",
        "  def init_weights(self):\n",
        "    self.W = np.random.uniform(-0.5 ,0.5 , 2000) #(2000, )\n",
        "    self.b = np.random.uniform(-0.5 ,0.5 , 1) #(1, )\n",
        "    self.bestW = np.random.uniform(-0.5 ,0.5 , 2000)\n",
        "\n",
        "\n",
        "  def train_model(self):\n",
        "    self.init_weights()\n",
        "    total_number_of_batches = int(self.X_train.shape[0] / self.batch_size)\n",
        "    self.loss_list = []\n",
        "    self.acc_train = []\n",
        "    self.acc_val = []\n",
        "    \n",
        "    prev_accuracy = 0\n",
        "\n",
        "    for i in range(self.num_epochs):\n",
        "      print(\"epoch \",i,\"-> \",end='')\n",
        "      total_loss = 0\n",
        "      for j in range(int(self.X_train.shape[0] / self.batch_size)):\n",
        "\n",
        "        X = self.X_train[self.batch_size*j : self.batch_size* (j+1)] # (batch_size, 2000)\n",
        "        y = self.y_train[self.batch_size*j : self.batch_size* (j+1)] # (batch_size, )\n",
        "\n",
        "        # calculating probabilities of X\n",
        "        h = (np.dot(X, self.W) + self.b) # (batch_size, )\n",
        "        prob = 1 / (1 + np.exp(-h)) # (batch_size, )\n",
        "        \n",
        "        # avoiding runtime warning for log(0)\n",
        "        for i in range(len(prob)):\n",
        "          if prob[i] == 0:\n",
        "            prob[i] = 10**(-10)\n",
        "          if prob[i] == 1:\n",
        "            prob[i] = 0.9999999999\n",
        "      \n",
        "        # calculating mean of loss for this batch\n",
        "        loss = np.mean( (-y * np.log(prob))  -  ((1-y) * np.log(1-prob)) )\n",
        "        # self.loss_list.append(loss)\n",
        "        total_loss += loss\n",
        "\n",
        "        # calculating gradient of loss for this batch (for weights and bias)\n",
        "        gradW = np.dot ((prob - y), X)\n",
        "        \n",
        "        # updating weights\n",
        "        self.W = self.W - self.learning_rate * gradW\n",
        "\n",
        "      #  get prediction for all train and validation data\n",
        "      predict_training = self.predict_label(self.X_train)\n",
        "      predict_validation = self.predict_label(self.X_val)\n",
        "      \n",
        "      # calculating training accuracy\n",
        "      counter = 0 \n",
        "      for i in range(len(predict_training)):\n",
        "        if predict_training[i] == self.y_train[i]:\n",
        "          counter += 1\n",
        "      train_acc = counter / len(predict_training)\n",
        "      self.acc_train.append(train_acc)\n",
        "      \n",
        "      # calculating validation accuracy\n",
        "      counter = 0 \n",
        "      for i in range(len(predict_validation)):\n",
        "        if predict_validation[i] == self.y_val[i]:\n",
        "          counter += 1\n",
        "      val_acc = counter / len(predict_validation)\n",
        "      self.acc_val.append(val_acc)\n",
        "                     \n",
        "      # use this list to create a plot for loss \n",
        "      self.loss_list.append(total_loss/total_number_of_batches)  \n",
        "      \n",
        "      print(\"train acc:\", train_acc, \"validation acc:\",val_acc, \"loss:\", total_loss/total_number_of_batches)\n",
        "      # compare with previous weights with respect to validation accuracy to find best model\n",
        "      if (prev_accuracy < val_acc):\n",
        "        prev_accuracy = val_acc\n",
        "        self.bestW = self.W\n",
        "  \n",
        "  def predict_label(self, x, bestW_flag = False):\n",
        "    \"\"\"make a prediction list of labels for x\"\"\"\n",
        "    if (not bestW_flag):\n",
        "      h = np.dot(x, self.W) + self.b\n",
        "      prob = 1 / (1 + np.exp(-h))\n",
        "      predict = np.empty_like(prob)\n",
        "      for i in range(len(prob)):\n",
        "        if prob[i] > self.threshhold :\n",
        "          predict[i] = 1\n",
        "        else:\n",
        "          predict[i] = 0\n",
        "    else:\n",
        "      h = np.dot(x, self.bestW) + self.b\n",
        "      prob = 1 / (1 + np.exp(-h))\n",
        "      predict = np.empty_like(prob)\n",
        "      for i in range(len(prob)):\n",
        "        if prob[i] > self.threshhold :\n",
        "          predict[i] = 1\n",
        "        else:\n",
        "          predict[i] = 0\n",
        "      \n",
        "    return predict\n",
        "    \n",
        "  def plot_loss(self):\n",
        "    \"\"\"plot the loss using loss list we gathered in training phase\"\"\"\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(self.loss_list)\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"loss\")\n",
        "  \n",
        "  def plot_accuracy(self):\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(self.acc_train, label= 'train accuracy')\n",
        "    plt.plot(self.acc_val, label= 'validation accuracy')    \n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    \n",
        "  def test_best_model(self):\n",
        "    predict_test = self.predict_label(self.X_test, bestW_flag = True)\n",
        "    counter = 0 \n",
        "    for i in range(len(predict_test)):\n",
        "      if predict_test[i] == self.y_test[i]:\n",
        "        counter += 1\n",
        "    test_accuracy = counter / len(predict_test)\n",
        "  \n",
        "    return test_accuracy\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b14BKZKo0pul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  lr = logistic_regression(batch_size = 20, learning_rate = 0.01, num_epochs = 300)\n",
        "  lr.set_training_data(x_train, y_train)\n",
        "  lr.set_validation_data(x_val, y_val)\n",
        "  lr.set_test_data(x_test, y_test)\n",
        "  lr.train_model()\n",
        "  lr.plot_loss()\n",
        "  lr.plot_accuracy()\n",
        "  print(\"test accuracy for best model: \", lr.test_best_model())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}